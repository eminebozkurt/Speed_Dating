# -*- coding: utf-8 -*-
"""Speed_Dating.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g5gj8RhyMRT8WaeFTjyOOUTlbVIbV85s
"""

import joblib
import pandas as pd
import seaborn as sns
import numpy as np
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
#!pip install catboost
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
import warnings
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
pd.set_option('display.max_rows', 500)
warnings.filterwarnings('ignore')

df_ = pd.read_csv('/content/sample_data/speed-dating.csv')
df = df_.copy()

df.head()

df.drop(columns=['has_null', 'wave', 'met', "expected_num_interested_in_me"], inplace=True)
d_cols = [col for col in df.columns if col.startswith('d') and col != 'd_age']
df.drop(columns=df[d_cols], inplace=True)
df.drop(columns=['interests_correlate'], inplace=True)

scaled_columns = [col for col in df.columns if col.startswith('pref') or col.endswith("important")]
df[scaled_columns] = MinMaxScaler(feature_range=(0, 10)).fit_transform(df[scaled_columns])

df.describe().T

def check_df(dataframe, head=5):
    print("##################### Shape #####################")
    print(dataframe.shape)
    print("##################### Types #####################")
    print(dataframe.dtypes)
    print("##################### Head #####################")
    print(dataframe.head(head))
    print("##################### Tail #####################")
    print(dataframe.tail(head))
    print("##################### NA #####################")
    print(dataframe.isnull().sum())
    print("##################### Quantiles #####################")
    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

def cat_summary(dataframe, col_name, plot=False):
    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
                        "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))
    print("##########################################")
    if plot:
        sns.countplot(x=dataframe[col_name], data=dataframe)
        plt.show(block=True)

def num_summary(dataframe, numerical_col, plot=False):
    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
    print(dataframe[numerical_col].describe(quantiles).T)

    if plot:
        dataframe[numerical_col].hist(bins=20)
        plt.xlabel(numerical_col)
        plt.title(numerical_col)
        plt.show(block=True)

def target_summary_with_num(dataframe, target, numerical_col):
    print(dataframe.groupby(target).agg({numerical_col: "mean"}), end="\n\n\n")

def target_summary_with_cat(dataframe, target, categorical_col):
    print(pd.DataFrame({"TARGET_MEAN": dataframe.groupby(categorical_col)[target].mean()}), end="\n\n\n")

def correlation_matrix(df, cols):
    fig = plt.gcf()
    fig.set_size_inches(10, 8)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    fig = sns.heatmap(df[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 5}, linecolor='w', cmap='RdBu')
    plt.show(block=True)
    # plt.savefig('corr.png')

def grab_col_names(dataframe, cat_th=10, car_th=20):
    """

    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.
    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.

    Parameters
    ------
        dataframe: dataframe
                Değişken isimleri alınmak istenilen dataframe
        cat_th: int, optional
                numerik fakat kategorik olan değişkenler için sınıf eşik değeri
        car_th: int, optinal
                kategorik fakat kardinal değişkenler için sınıf eşik değeri

    Returns
    ------
        cat_cols: list
                Kategorik değişken listesi
        num_cols: list
                Numerik değişken listesi
        cat_but_car: list
                Kategorik görünümlü kardinal değişken listesi

    Examples
    ------
        import seaborn as sns
        df = sns.load_dataset("iris")
        print(grab_col_names(df))


    Notes
    ------
        cat_cols + num_cols + cat_but_car = toplam değişken sayısı
        num_but_cat cat_cols'un içerisinde.
        Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı

    """

    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    # print(f"Observations: {dataframe.shape[0]}")
    # print(f"Variables: {dataframe.shape[1]}")
    # print(f'cat_cols: {len(cat_cols)}')
    # print(f'num_cols: {len(num_cols)}')
    # print(f'cat_but_car: {len(cat_but_car)}')
    # print(f'num_but_cat: {len(num_but_cat)}')
    return cat_cols, num_cols, cat_but_car

check_df(df)

# Kategorik değişkenlerin incelenmesi
for col in cat_cols:
    cat_summary(df, col, True)

for col in num_cols:
    num_summary(df, col, True)

# Target ile sayısal değişkenlerin incelemesi
for col in num_cols:
    target_summary_with_num(df, "match", col)

# Sayısal değişkenkerin birbirleri ile korelasyonu
correlation_matrix(df, num_cols)

cat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)

cat_cols

################################################
# 2. Data Preprocessing & Feature Engineering
################################################

def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

def check_outlier(dataframe, col_name, q1=0.05, q3=0.95):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
    return dataframe

###############################
# eksik degerlerin temizlenmesi
###############################

def quick_missing_imp(data, num_method="median", cat_length=20, target="match"):
    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Eksik değere sahip olan değişkenler listelenir

    temp_target = data[target]

    print("# BEFORE")
    print(data[variables_with_na].isnull().sum(), "\n\n")  # Uygulama öncesi değişkenlerin eksik değerlerinin sayısı

    # değişken object ve sınıf sayısı cat_lengthe eşit veya altındaysa boş değerleri mode ile doldur
    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == "O" and len(x.unique()) <= cat_length) else x, axis=0)

    # num_method mean ise tipi object olmayan değişkenlerin boş değerleri ortalama ile dolduruluyor
    if num_method == "mean":
        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != "O" else x, axis=0)
    # num_method median ise tipi object olmayan değişkenlerin boş değerleri ortalama ile dolduruluyor
    elif num_method == "median":
        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)

    data[target] = temp_target

    print("# AFTER \n Imputation method is 'MODE' for categorical variables!")
    print(" Imputation method is '" + num_method.upper() + "' for numeric variables! \n")
    print(data[variables_with_na].isnull().sum(), "\n\n")

    return data

df = quick_missing_imp(df, num_method="mean", cat_length=20)

[col for col in df.columns if df[col].isnull().sum() > 0]

replace_with_thresholds(df, 'age')
replace_with_thresholds(df, 'age_o')
replace_with_thresholds(df, 'd_age')
replace_with_thresholds(df, 'pref_o_ambitious')
replace_with_thresholds(df, 'ambtition_important')

df['field'] = df['field'].str.lower()
df['field'] = df['field'].str.replace("'", "", regex=False)
df['field'] = df['field'].str.replace(" ", "_", regex=False)
df['field'] = df['field'].str.replace("[", "(", regex=False)
df['field'] = df['field'].str.replace("]", ")", regex=False)
df['field'] = df['field'].fillna('unknown')
df['field'] = df['field'].astype(str)


df['field'] = df['field'].str.replace('business-_mba', 'business_(mba)', regex=False)
df['field'] = df['field'].str.replace('business/law', 'business_(law)', regex=False)
df['field'] = df['field'].str.replace('business;_marketing', 'business_(marketing)', regex=False)
df['field'] = df['field'].str.replace('business;_media', 'business_(media)', regex=False)
df['field'] = df['field'].str.replace('business/_finance/_real_estate', 'business_(finance_&_real_estate)', regex=False)
df['field'] = df['field'].str.replace('creative_writing_-_nonfiction', 'creative_writing_(nonfiction)', regex=False)
df['field'] = df['field'].str.replace('climate-earth_and_environ._science', 'earth_and_environmental_science', regex=False)
df['field'] = df['field'].str.replace('electrical_engg.', 'electrical_engineering', regex=False)
df['field'] = df['field'].str.replace('finanace', 'finance', regex=False)
df['field'] = df['field'].str.replace('finance&economics', 'finance_&_economics', regex=False)
df['field'] = df['field'].str.replace('finance/economics', 'finance_&_economics', regex=False)
df['field'] = df['field'].str.replace('international_affairs/business', 'international_affairs_(business)', regex=False)
df['field'] = df['field'].str.replace('international_affairs/finance', 'international_affairs_(finance)', regex=False)
df['field'] = df['field'].str.replace('international_affairs/international_finance', 'international_affairs_(finance)', regex=False)
df['field'] = df['field'].str.replace('intrernational_affairs', 'international_affairs', regex=False)
df['field'] = df['field'].str.replace('master_in_public_administration', 'masters_in_public_administration', regex=False)
df['field'] = df['field'].str.replace('master_of_international_affairs', 'masters_in_international_affairs', regex=False)
df['field'] = df['field'].str.replace('math', 'mathematics', regex=False)
df['field'] = df['field'].str.replace('mfa__poetry', 'mfa_poetry', regex=False)
df['field'] = df['field'].str.replace('mfa_-film', 'mfa_film', regex=False)
df['field'] = df['field'].str.replace('nutritiron', 'nutrition', regex=False)
df['field'] = df['field'].str.replace('sipa_/_mia', 'masters_in_international_affairs', regex=False)
df['field'] = df['field'].str.replace('sipa-international_affairs', 'international_affairs', regex=False)
df['field'] = df['field'].str.replace('sociomedical_sciences-_school_of_public_health', 'sociomedical_sciences', regex=False)
df['field'] = df['field'].str.replace('speech_languahe_pathology', 'speech_pathology', regex=False)
df['field'] = df['field'].str.replace('speech_language_pathology', 'speech_pathology', regex=False)
df['field'] = df['field'].str.replace('stats', 'statistics', regex=False)
df['field'] = df['field'].str.replace('tc_(health_ed)', 'health_education', regex=False)



df['field'] = df['field'].str.replace('economics;_english', 'economics_english', regex=False)
df['field'] = df['field'].str.replace('economics;_sociology', 'economics_sociology', regex=False)
df['field'] = df['field'].str.replace('education-_literacy_specialist', 'education_literacy_specialist', regex=False)
df['field'] = df['field'].str.replace('education_leadership_-_public_school_administration', 'education_leadership_public_school_administration', regex=False)
df['field'] = df['field'].str.replace('elementary_education_-_preservice', 'elementary_education_preservice', regex=False)
df['field'] = df['field'].str.replace('higher_ed._-_m.a.', 'higher_ed._m.a.', regex=False)
df['field'] = df['field'].str.replace('history_(gsas_-_phd)', 'history_(gsas_phd)', regex=False)
df['field'] = df['field'].str.replace('international_affairs_-_economic_development', 'international_affairs_economic_development', regex=False)
df['field'] = df['field'].str.replace('international_affairs_-_economic_policy', 'international_affairs_economic_policy', regex=False)
df['field'] = df['field'].str.replace('international_finance;_economic_policy', 'international_finance_economic_policy', regex=False)
df['field'] = df['field'].str.replace('international_security_policy_-_sipa', 'international_security_policy_sipa', regex=False)
df['field'] = df['field'].str.replace('mba_-_private_equity_/_real_estate', 'mba_private_equity_/_real_estate', regex=False)
df['field'] = df['field'].str.replace('religion;_gsas', 'religion_gsas', regex=False)
df['field'] = df['field'].str.replace('sipa_-_energy', 'sipa_energy', regex=False)
df['field'] = df['field'].str.replace('religion;_gsas', 'religion_gsas', regex=False)
df['field'] = df['field'].str.replace('soa_--_writing', 'soa_writing', regex=False)
df['field'] = df['field'].str.replace('undergrad_-_gs', 'undergrad_gs', regex=False)


df.loc[df['field'].str.contains("law"), "field_cd"] = "Law"
df.loc[df['field'].str.contains("(international|political|business|finance|economics|mba)", regex=True), "field_cd"] = "Economic and Administrative Sciences"
df.loc[df['field'].str.contains("(social|psychology|history|finance|economics|philosophy)", regex=True), "field_cd"] = "Human Sciences"
df.loc[df['field'].str.contains("(bio|mathematic|physics|statistics|chemistry|climate)", regex=True), "field_cd"] = "Sciences"
df.loc[df['field'].str.contains("(industrial|engineering|computer|mechanical|chemistry)", regex=True), "field_cd"] = "Engineering"
df.loc[df['field'].str.contains("education"), "field_cd"] = "Education"
df.loc[df['field'].str.contains("(acting|film|art|journalism|writing)", regex=True), "field_cd"] = "Fine Arts"
df["field_cd"].fillna("Others", inplace=True)

df.loc[(df['age'] <= 26), "new_age_cat"] = 'lower'
df.loc[(df['age'] > 26), "new_age_cat"] = 'upper'

df.loc[(df['age_o'] <= 26), "new_age_cat"] = 'lower'
df.loc[(df['age_o'] > 26), "new_age_cat"] = 'upper'

for col in num_cols:
    col_name = 'new_' + col
    df[col_name] = pd.qcut(df[col], 5, labels=False, duplicates='drop')



df["new_importance_race_religion"] = df["importance_same_race"] * df["importance_same_religion"]
df["new_importance_race_religion_cat"] = pd.qcut(df["new_importance_race_religion"], 4, labels=False, duplicates='drop')

df["new_cultural_activity_importance"] = df["theater"] + df["movies"] + df["concerts"] + df["museums"]
df["new_cultural_activity_importance_cat"] = pd.qcut(df["new_importance_race_religion"], 4, labels=False, duplicates='drop')

df["new_self_entertain_importance"] = df["music"] + df["tv"] + df["reading"] + df["shopping"]
df["new_self_entertain_importance_cat"] = pd.qcut(df["new_self_entertain_importance"], 4, labels=False, duplicates='drop')

df["new_sports_importance"] = df["sports"] + df["exercise"] + df["hiking"] + df["yoga"]
df["new_sports_importance_cat"] = pd.qcut(df["new_sports_importance"], 4, labels=False, duplicates='drop')


df.loc[(df["age_o"] > df["age"]) & (df["gender"] == "female"), "age_diff_cat"] = "younger_female"
df.loc[(df["age_o"] > df["age"]) & (df["gender"] == "male"), "age_diff_cat"] = "younger_male"
df.loc[(df["age_o"] < df["age"]) & (df["gender"] == "male"), "age_diff_cat"] = "older_male"
df.loc[(df["age_o"] < df["age"]) & (df["gender"] == "female"), "age_diff_cat"] = "older_female"
df.loc[(df["d_age"] == 0), "age_diff_cat"] = "same_age"

df = quick_missing_imp(df, num_method="mean", cat_length=20)

df.head()

cat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)

for col in num_cols:
  if df[col].nunique() > 1 and df[col].nunique() < 7:
    print(col)
    print(df[col].value_counts())

cat_cols = [col for col in cat_cols if col!= "match"]
ohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]
one_hot_encoder(df, ohe_cols).head()
df = one_hot_encoder(df, cat_cols, drop_first=True)

######################################################
# 3. Base Models
######################################################


def base_models(X, y, scoring="roc_auc"):
    print("Base Models....")
    classifiers = [('LR', LogisticRegression()),
                   ('KNN', KNeighborsClassifier()),
                   ("SVC", SVC()),
                   ("CART", DecisionTreeClassifier()),
                   ("RF", RandomForestClassifier()),
                   ('Adaboost', AdaBoostClassifier()),
                   ('GBM', GradientBoostingClassifier()),
                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),
                   ('LightGBM', LGBMClassifier()),
                   ('CatBoost', CatBoostClassifier(verbose=False))
                   ]

    for name, classifier in classifiers:
        cv_results = cross_validate(classifier, X, y, cv=10, scoring=scoring)
        print(f"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) ")

y = df["match"]
X = df.drop(["match", "field"], axis=1)

base_models(X, y, scoring="roc_auc")

######################################################
# 4. Automated Hyperparameter Optimization
######################################################

knn_params = {"n_neighbors": [33]}

# KNN best params: {'n_neighbors': 33}
# KNN best params: {'n_neighbors': 33}


cart_params = {'max_depth': [3],
               "min_samples_split": [30]}

#cart_params = {'max_depth': [3, 4, 5, 6, 7],
#               "min_samples_split": [30, 33, 36, 39, 42, 45]}
        
# CART best params: {'max_depth': 5, 'min_samples_split': 30}
# CART best params: {'max_depth': 3, 'min_samples_split': 36}

rf_params = {"max_depth": [15],
             "max_features": ["auto"],
             "min_samples_split": [25],
             "n_estimators": [400]}

# rf_params = {"max_depth": [15, 18, 25],
#              "max_features": [5, 7, "auto"],
#              "min_samples_split": [15, 20, 25],
#              "n_estimators": [300, 400]}

# RF best params: {'max_depth': 15, 'max_features': 'auto', 'min_samples_split': 20, 'n_estimators': 300}
# RF best params: {'max_depth': 25, 'max_features': 'auto', 'min_samples_split': 15, 'n_estimators': 400}


xgboost_params = {"learning_rate": [0.01],
                  "max_depth": [3],
                  "n_estimators": [300]}

# xgboost_params = {"learning_rate": [0.1, 0.01],
#                   "max_depth": [3, 5, 7],
#                   "n_estimators": [200, 300]}

# XGBoost best params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}
# XGBoost best params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}

lightgbm_params = {"learning_rate": [0.01],
                   "n_estimators": [400],
                   "max_depth": [3]}

# lightgbm_params = {"learning_rate": [0.01],
#                    "n_estimators": [200, 300, 400]}

# LightGBM best params: {'learning_rate': 0.01, 'n_estimators': 300}
# LightGBM best params: {'learning_rate': 0.01, 'n_estimators': 400}

classifiers = [('KNN', KNeighborsClassifier(), knn_params),
               ("CART", DecisionTreeClassifier(), cart_params),
               ("RF", RandomForestClassifier(), rf_params),
               #("GBM", GradientBoostingClassifier(), rf_params),
               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),
               ('LightGBM', LGBMClassifier(), lightgbm_params)]

def hyperparameter_optimization(X, y, cv=5, scoring="roc_auc"):
    print("Hyperparameter Optimization....")
    best_models = {}
    for name, classifier, params in classifiers:
        print(f"########## {name} ##########")
        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)
        print(f"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}")

        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)
        final_model = classifier.set_params(**gs_best.best_params_)

        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)
        print(f"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}")
        print(f"{name} best params: {gs_best.best_params_}", end="\n\n")
        best_models[name] = final_model
    return best_models

best_models = hyperparameter_optimization(X, y)

######################################################
# 5. Stacking & Ensemble Learning
######################################################

def voting_classifier(best_models, X, y):
    print("Voting Classifier...")

    voting_clf = VotingClassifier(estimators=[('RF', best_models["RF"]),
                                              ('XGBoost', best_models["XGBoost"]),
                                              ('LightGBM', best_models["LightGBM"])],
                                  voting='soft').fit(X, y)

    cv_results = cross_validate(voting_clf, X, y, cv=5, scoring=["accuracy", "f1", "roc_auc"])
    #print(f"Accuracy: {cv_results['test_accuracy'].mean()}")
    #print(f"F1Score: {cv_results['test_f1'].mean()}")
    print(f"ROC_AUC: {cv_results['test_roc_auc'].mean()}")
    return voting_clf

def compute_feature_importance(voting_clf):
    """ Function to compute feature importance of Voting Classifier """

    feature_importance = dict()
    for est in voting_clf.estimators_:
        feature_importance[str(est)] = est.feature_importances_

    fe_scores = [0] * len(list(feature_importance.values())[0])
    for idx, imp_score in enumerate(feature_importance.values()):
        #imp_score_with_weight = imp_score * weights[idx]
        fe_scores = list(np.add(fe_scores, list(imp_score)))
    return fe_scores

def plot_feature_importance(importance,names,model_type):
    # Create arrays from feature importance and feature names
    feature_importance = np.array(importance)
    feature_names = np.array(names)

    # Create a DataFrame using a Dictionary
    data = {'feature_names': feature_names, 'feature_importance': feature_importance}
    fi_df = pd.DataFrame(data)

    # Sort the DataFrame in order decreasing feature importance
    fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)

    # Define size of bar plot
    plt.figure(figsize=(15, 10))
    # Plot Searborn bar chart
    sns.barplot(x=fi_df['feature_importance'].head(10), y=fi_df['feature_names'].head(10))
    # Add chart labels
    plt.title(model_type + ' FEATURE IMPORTANCE')
    plt.xlabel('FEATURE IMPORTANCE')
    plt.ylabel('FEATURE NAMES')
    plt.show()

voting_clf = voting_classifier(best_models, X, y)

feature_scores = compute_feature_importance(voting_clf)

plot_feature_importance(feature_scores, X.columns, 'VotingClassifier')

random_user = X.sample(1)
voting_clf.predict(random_user)

random_user

df["match"][8345]

random_user = X.sample(1)
voting_clf.predict(random_user)